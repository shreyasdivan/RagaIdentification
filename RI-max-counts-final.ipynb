{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "from os import walk\n",
    "import librosa\n",
    "import librosa.display\n",
    "from pydub import AudioSegment\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signal2, _ = librosa.load(\"C:/Users/sdiva/Documents/Python_scripts/RI/audio/yaman_wav/flute.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROW_DURATION = 10\n",
    "SAMPLING_RATE=22050\n",
    "FRAME_SIZE = 2048\n",
    "HOP = 2048\n",
    "SAMPLE_ROW_LEN = ROW_DURATION*SAMPLING_RATE\n",
    "JUMP = SAMPLING_RATE*5\n",
    "BINS = librosa.fft_frequencies(sr=SAMPLING_RATE, n_fft=FRAME_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_x_row(signal,sample_row_len,rate,jump,bins):\n",
    "    \"\"\"\n",
    "    Takes audio signal and generates sample rows wrt ROW_DURATION and JUMP\n",
    "    i.e, if audio signal is of 20 sec, ROW_DURATION = 10 sec, JUMP = 2 sec,\n",
    "    Total 6 rows will be generated from the audio signal - 0-10 sec, 2-12 sec, ... 10-20 sec \n",
    "    i.e of each 10 sec\n",
    "    \"\"\"\n",
    "    num = len(signal)//sample_row_len\n",
    "    rows = []\n",
    "    for i in range(0,(num-1)*sample_row_len,jump):\n",
    "        r = signal[i:i+sample_row_len]\n",
    "        r_spec = convert_to_spec(r,FRAME_SIZE,HOP)\n",
    "        max_f = r_spec.argmax(axis=0)\n",
    "        count_r = np.zeros(len(bins))\n",
    "        for j in max_f:\n",
    "            count_r[j]+=1\n",
    "        rows.append(count_r)\n",
    "    return rows\n",
    "\n",
    "def convert_to_spec(row,frame_size,hop):\n",
    "    stft = librosa.stft(row,n_fft=frame_size, hop_length=hop,center=False)\n",
    "    spec = np.abs(stft)\n",
    "    return spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataset from folder paths\n",
    "\n",
    "paths = ['C:/Users/sdiva/Documents/Python_scripts/RI/audio/train/yaman_wav2/',\n",
    "         'C:/Users/sdiva/Documents/Python_scripts/RI/audio/train/bhairavi_wav2/']\n",
    "\n",
    "x_data = []\n",
    "len_perclass = []\n",
    "for path in paths:\n",
    "    f = []\n",
    "    for (dirpath, dirnames, filenames) in walk(path):\n",
    "        f.extend(filenames)\n",
    "        break\n",
    "    for file in f:\n",
    "        s, _ = librosa.load(path+file,sr = SAMPLING_RATE)\n",
    "        row = create_x_row(s,SAMPLE_ROW_LEN,SAMPLING_RATE,JUMP,BINS)\n",
    "        x_data.extend(row)\n",
    "    len_perclass.append(len(x_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6528, 1025)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data = np.array(x_data)\n",
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3578, 6528]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking class distribution\n",
    "\n",
    "len_perclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating labels : 0 for Yaman, 1 for Bhairavi\n",
    "\n",
    "y0 = np.zeros(len_perclass[0]).reshape(-1,1)\n",
    "y1 = np.ones(len_perclass[1]-len_perclass[0]).reshape(-1,1)\n",
    "y_data = np.vstack((y0,y1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1025,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('C:/Users/sdiva/Documents/Python_scripts/RI/x_data.pickle', 'wb') as x:\n",
    "#    pickle.dump(x_data, x)\n",
    "\n",
    "# with open('C:/Users/sdiva/Documents/Python_scripts/RI/y_data.pickle', 'wb') as y:\n",
    "#    pickle.dump(y_data, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.25, stratify=y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 1025)]            0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 2)                 2052      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 4,293\n",
      "Trainable params: 4,293\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_seq = layers.Input(shape=(1025),dtype='float32')\n",
    "x = layers.Dense(2, kernel_initializer=initializers.RandomUniform(),activation='relu')(input_seq)\n",
    "x = layers.Dense(32, kernel_initializer=initializers.RandomUniform(),activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(32, kernel_initializer=initializers.RandomUniform(),activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(32, kernel_initializer=initializers.RandomUniform(),activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "preds = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = tf.keras.Model(input_seq, preds)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "153/153 [==============================] - 1s 2ms/step - loss: 0.2095 - accuracy: 0.9187 - val_loss: 0.3272 - val_accuracy: 0.8866\n",
      "Epoch 2/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2072 - accuracy: 0.9203 - val_loss: 0.3264 - val_accuracy: 0.8860\n",
      "Epoch 3/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2077 - accuracy: 0.9187 - val_loss: 0.3258 - val_accuracy: 0.8866\n",
      "Epoch 4/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2027 - accuracy: 0.9208 - val_loss: 0.3255 - val_accuracy: 0.8866\n",
      "Epoch 5/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2019 - accuracy: 0.9201 - val_loss: 0.3251 - val_accuracy: 0.8873\n",
      "Epoch 6/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2013 - accuracy: 0.9201 - val_loss: 0.3246 - val_accuracy: 0.8873\n",
      "Epoch 7/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2031 - accuracy: 0.9201 - val_loss: 0.3242 - val_accuracy: 0.8866\n",
      "Epoch 8/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2024 - accuracy: 0.9218 - val_loss: 0.3241 - val_accuracy: 0.8873\n",
      "Epoch 9/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2035 - accuracy: 0.9203 - val_loss: 0.3239 - val_accuracy: 0.8873\n",
      "Epoch 10/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2027 - accuracy: 0.9212 - val_loss: 0.3237 - val_accuracy: 0.8873\n",
      "Epoch 11/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2018 - accuracy: 0.9232 - val_loss: 0.3237 - val_accuracy: 0.8873\n",
      "Epoch 12/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2022 - accuracy: 0.9208 - val_loss: 0.3235 - val_accuracy: 0.8873\n",
      "Epoch 13/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2078 - accuracy: 0.9214 - val_loss: 0.3233 - val_accuracy: 0.8873\n",
      "Epoch 14/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2022 - accuracy: 0.9214 - val_loss: 0.3232 - val_accuracy: 0.8879\n",
      "Epoch 15/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1987 - accuracy: 0.9230 - val_loss: 0.3232 - val_accuracy: 0.8879\n",
      "Epoch 16/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2063 - accuracy: 0.9212 - val_loss: 0.3228 - val_accuracy: 0.8879\n",
      "Epoch 17/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2046 - accuracy: 0.9205 - val_loss: 0.3228 - val_accuracy: 0.8879\n",
      "Epoch 18/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2025 - accuracy: 0.9208 - val_loss: 0.3227 - val_accuracy: 0.8879\n",
      "Epoch 19/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2055 - accuracy: 0.9216 - val_loss: 0.3226 - val_accuracy: 0.8879\n",
      "Epoch 20/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2012 - accuracy: 0.9236 - val_loss: 0.3225 - val_accuracy: 0.8879\n",
      "Epoch 21/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2004 - accuracy: 0.9216 - val_loss: 0.3225 - val_accuracy: 0.8879\n",
      "Epoch 22/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2015 - accuracy: 0.9232 - val_loss: 0.3224 - val_accuracy: 0.8879\n",
      "Epoch 23/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2043 - accuracy: 0.9228 - val_loss: 0.3223 - val_accuracy: 0.8879\n",
      "Epoch 24/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1996 - accuracy: 0.9238 - val_loss: 0.3223 - val_accuracy: 0.8879\n",
      "Epoch 25/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2040 - accuracy: 0.9226 - val_loss: 0.3224 - val_accuracy: 0.8879\n",
      "Epoch 26/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2053 - accuracy: 0.9232 - val_loss: 0.3223 - val_accuracy: 0.8879\n",
      "Epoch 27/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2026 - accuracy: 0.9228 - val_loss: 0.3222 - val_accuracy: 0.8879\n",
      "Epoch 28/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2037 - accuracy: 0.9226 - val_loss: 0.3220 - val_accuracy: 0.8879\n",
      "Epoch 29/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2033 - accuracy: 0.9230 - val_loss: 0.3219 - val_accuracy: 0.8873\n",
      "Epoch 30/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2013 - accuracy: 0.9234 - val_loss: 0.3220 - val_accuracy: 0.8879\n",
      "Epoch 31/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2012 - accuracy: 0.9232 - val_loss: 0.3221 - val_accuracy: 0.8879\n",
      "Epoch 32/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2062 - accuracy: 0.9208 - val_loss: 0.3219 - val_accuracy: 0.8873\n",
      "Epoch 33/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1994 - accuracy: 0.9236 - val_loss: 0.3219 - val_accuracy: 0.8873\n",
      "Epoch 34/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2018 - accuracy: 0.9240 - val_loss: 0.3219 - val_accuracy: 0.8873\n",
      "Epoch 35/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2074 - accuracy: 0.9230 - val_loss: 0.3218 - val_accuracy: 0.8873\n",
      "Epoch 36/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2008 - accuracy: 0.9222 - val_loss: 0.3219 - val_accuracy: 0.8873\n",
      "Epoch 37/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2029 - accuracy: 0.9238 - val_loss: 0.3220 - val_accuracy: 0.8885\n",
      "Epoch 38/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2044 - accuracy: 0.9228 - val_loss: 0.3220 - val_accuracy: 0.8885\n",
      "Epoch 39/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2042 - accuracy: 0.9222 - val_loss: 0.3220 - val_accuracy: 0.8879\n",
      "Epoch 40/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2011 - accuracy: 0.9222 - val_loss: 0.3221 - val_accuracy: 0.8879\n",
      "Epoch 41/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2001 - accuracy: 0.9226 - val_loss: 0.3222 - val_accuracy: 0.8885\n",
      "Epoch 42/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2056 - accuracy: 0.9226 - val_loss: 0.3221 - val_accuracy: 0.8885\n",
      "Epoch 43/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2013 - accuracy: 0.9226 - val_loss: 0.3221 - val_accuracy: 0.8879\n",
      "Epoch 44/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2018 - accuracy: 0.9238 - val_loss: 0.3223 - val_accuracy: 0.8885\n",
      "Epoch 45/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1984 - accuracy: 0.9232 - val_loss: 0.3225 - val_accuracy: 0.8891\n",
      "Epoch 46/200\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.2036 - accuracy: 0.9228 - val_loss: 0.3226 - val_accuracy: 0.8891\n",
      "Epoch 47/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1982 - accuracy: 0.9222 - val_loss: 0.3226 - val_accuracy: 0.8885\n",
      "Epoch 48/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1991 - accuracy: 0.9236 - val_loss: 0.3225 - val_accuracy: 0.8885\n",
      "Epoch 49/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1998 - accuracy: 0.9236 - val_loss: 0.3225 - val_accuracy: 0.8885\n",
      "Epoch 50/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2028 - accuracy: 0.9224 - val_loss: 0.3226 - val_accuracy: 0.8885\n",
      "Epoch 51/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1984 - accuracy: 0.9238 - val_loss: 0.3227 - val_accuracy: 0.8885\n",
      "Epoch 52/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1987 - accuracy: 0.9222 - val_loss: 0.3228 - val_accuracy: 0.8879\n",
      "Epoch 53/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2019 - accuracy: 0.9228 - val_loss: 0.3226 - val_accuracy: 0.8879\n",
      "Epoch 54/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2033 - accuracy: 0.9234 - val_loss: 0.3224 - val_accuracy: 0.8873\n",
      "Epoch 55/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1994 - accuracy: 0.9218 - val_loss: 0.3223 - val_accuracy: 0.8879\n",
      "Epoch 56/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1995 - accuracy: 0.9240 - val_loss: 0.3224 - val_accuracy: 0.8879\n",
      "Epoch 57/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1987 - accuracy: 0.9236 - val_loss: 0.3225 - val_accuracy: 0.8879\n",
      "Epoch 58/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2025 - accuracy: 0.9234 - val_loss: 0.3225 - val_accuracy: 0.8885\n",
      "Epoch 59/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2049 - accuracy: 0.9242 - val_loss: 0.3224 - val_accuracy: 0.8885\n",
      "Epoch 60/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2003 - accuracy: 0.9224 - val_loss: 0.3224 - val_accuracy: 0.8885\n",
      "Epoch 61/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1999 - accuracy: 0.9228 - val_loss: 0.3224 - val_accuracy: 0.8885\n",
      "Epoch 62/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1984 - accuracy: 0.9240 - val_loss: 0.3225 - val_accuracy: 0.8885\n",
      "Epoch 63/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2021 - accuracy: 0.9242 - val_loss: 0.3225 - val_accuracy: 0.8885\n",
      "Epoch 64/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1978 - accuracy: 0.9248 - val_loss: 0.3228 - val_accuracy: 0.8885\n",
      "Epoch 65/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1993 - accuracy: 0.9250 - val_loss: 0.3229 - val_accuracy: 0.8885\n",
      "Epoch 66/200\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.2014 - accuracy: 0.9234 - val_loss: 0.3230 - val_accuracy: 0.8885\n",
      "Epoch 67/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2006 - accuracy: 0.9226 - val_loss: 0.3228 - val_accuracy: 0.8885\n",
      "Epoch 68/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2007 - accuracy: 0.9236 - val_loss: 0.3227 - val_accuracy: 0.8885\n",
      "Epoch 69/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2002 - accuracy: 0.9222 - val_loss: 0.3226 - val_accuracy: 0.8885\n",
      "Epoch 70/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1999 - accuracy: 0.9238 - val_loss: 0.3226 - val_accuracy: 0.8885\n",
      "Epoch 71/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1994 - accuracy: 0.9240 - val_loss: 0.3226 - val_accuracy: 0.8885\n",
      "Epoch 72/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2008 - accuracy: 0.9236 - val_loss: 0.3227 - val_accuracy: 0.8885\n",
      "Epoch 73/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1997 - accuracy: 0.9226 - val_loss: 0.3228 - val_accuracy: 0.8885\n",
      "Epoch 74/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1997 - accuracy: 0.9230 - val_loss: 0.3228 - val_accuracy: 0.8885\n",
      "Epoch 75/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2019 - accuracy: 0.9234 - val_loss: 0.3230 - val_accuracy: 0.8885\n",
      "Epoch 76/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1993 - accuracy: 0.9244 - val_loss: 0.3229 - val_accuracy: 0.8885\n",
      "Epoch 77/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2017 - accuracy: 0.9234 - val_loss: 0.3230 - val_accuracy: 0.8885\n",
      "Epoch 78/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2002 - accuracy: 0.9244 - val_loss: 0.3232 - val_accuracy: 0.8885\n",
      "Epoch 79/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1982 - accuracy: 0.9238 - val_loss: 0.3232 - val_accuracy: 0.8885\n",
      "Epoch 80/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1970 - accuracy: 0.9248 - val_loss: 0.3234 - val_accuracy: 0.8885\n",
      "Epoch 81/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2019 - accuracy: 0.9234 - val_loss: 0.3235 - val_accuracy: 0.8885\n",
      "Epoch 82/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2021 - accuracy: 0.9236 - val_loss: 0.3235 - val_accuracy: 0.8885\n",
      "Epoch 83/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2000 - accuracy: 0.9228 - val_loss: 0.3237 - val_accuracy: 0.8885\n",
      "Epoch 84/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2016 - accuracy: 0.9232 - val_loss: 0.3236 - val_accuracy: 0.8885\n",
      "Epoch 85/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1993 - accuracy: 0.9230 - val_loss: 0.3237 - val_accuracy: 0.8885\n",
      "Epoch 86/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2019 - accuracy: 0.9238 - val_loss: 0.3238 - val_accuracy: 0.8885\n",
      "Epoch 87/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2009 - accuracy: 0.9230 - val_loss: 0.3237 - val_accuracy: 0.8885\n",
      "Epoch 88/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2028 - accuracy: 0.9228 - val_loss: 0.3239 - val_accuracy: 0.8885\n",
      "Epoch 89/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2016 - accuracy: 0.9238 - val_loss: 0.3239 - val_accuracy: 0.8885\n",
      "Epoch 90/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2024 - accuracy: 0.9240 - val_loss: 0.3240 - val_accuracy: 0.8885\n",
      "Epoch 91/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1993 - accuracy: 0.9228 - val_loss: 0.3240 - val_accuracy: 0.8885\n",
      "Epoch 92/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1996 - accuracy: 0.9242 - val_loss: 0.3240 - val_accuracy: 0.8885\n",
      "Epoch 93/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2002 - accuracy: 0.9240 - val_loss: 0.3240 - val_accuracy: 0.8885\n",
      "Epoch 94/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1983 - accuracy: 0.9228 - val_loss: 0.3241 - val_accuracy: 0.8885\n",
      "Epoch 95/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2022 - accuracy: 0.9240 - val_loss: 0.3240 - val_accuracy: 0.8879\n",
      "Epoch 96/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1982 - accuracy: 0.9240 - val_loss: 0.3240 - val_accuracy: 0.8879\n",
      "Epoch 97/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1984 - accuracy: 0.9234 - val_loss: 0.3242 - val_accuracy: 0.8879\n",
      "Epoch 98/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1996 - accuracy: 0.9238 - val_loss: 0.3243 - val_accuracy: 0.8879\n",
      "Epoch 99/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2020 - accuracy: 0.9244 - val_loss: 0.3245 - val_accuracy: 0.8879\n",
      "Epoch 100/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1993 - accuracy: 0.9242 - val_loss: 0.3245 - val_accuracy: 0.8879\n",
      "Epoch 101/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2015 - accuracy: 0.9244 - val_loss: 0.3245 - val_accuracy: 0.8879\n",
      "Epoch 102/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1980 - accuracy: 0.9240 - val_loss: 0.3246 - val_accuracy: 0.8885\n",
      "Epoch 103/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1996 - accuracy: 0.9244 - val_loss: 0.3246 - val_accuracy: 0.8879\n",
      "Epoch 104/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2002 - accuracy: 0.9240 - val_loss: 0.3246 - val_accuracy: 0.8879\n",
      "Epoch 105/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1983 - accuracy: 0.9250 - val_loss: 0.3249 - val_accuracy: 0.8885\n",
      "Epoch 106/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1998 - accuracy: 0.9238 - val_loss: 0.3246 - val_accuracy: 0.8879\n",
      "Epoch 107/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2031 - accuracy: 0.9236 - val_loss: 0.3248 - val_accuracy: 0.8879\n",
      "Epoch 108/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1982 - accuracy: 0.9246 - val_loss: 0.3251 - val_accuracy: 0.8885\n",
      "Epoch 109/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1983 - accuracy: 0.9252 - val_loss: 0.3252 - val_accuracy: 0.8891\n",
      "Epoch 110/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2024 - accuracy: 0.9244 - val_loss: 0.3249 - val_accuracy: 0.8879\n",
      "Epoch 111/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2027 - accuracy: 0.9246 - val_loss: 0.3249 - val_accuracy: 0.8879\n",
      "Epoch 112/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2018 - accuracy: 0.9242 - val_loss: 0.3252 - val_accuracy: 0.8885\n",
      "Epoch 113/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1986 - accuracy: 0.9254 - val_loss: 0.3251 - val_accuracy: 0.8879\n",
      "Epoch 114/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2023 - accuracy: 0.9226 - val_loss: 0.3253 - val_accuracy: 0.8879\n",
      "Epoch 115/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2022 - accuracy: 0.9240 - val_loss: 0.3254 - val_accuracy: 0.8873\n",
      "Epoch 116/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1979 - accuracy: 0.9246 - val_loss: 0.3254 - val_accuracy: 0.8879\n",
      "Epoch 117/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1988 - accuracy: 0.9242 - val_loss: 0.3256 - val_accuracy: 0.8879\n",
      "Epoch 118/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1992 - accuracy: 0.9250 - val_loss: 0.3259 - val_accuracy: 0.8885\n",
      "Epoch 119/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2004 - accuracy: 0.9238 - val_loss: 0.3260 - val_accuracy: 0.8885\n",
      "Epoch 120/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2046 - accuracy: 0.9246 - val_loss: 0.3261 - val_accuracy: 0.8885\n",
      "Epoch 121/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1955 - accuracy: 0.9240 - val_loss: 0.3261 - val_accuracy: 0.8885\n",
      "Epoch 122/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1956 - accuracy: 0.9248 - val_loss: 0.3260 - val_accuracy: 0.8879\n",
      "Epoch 123/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1982 - accuracy: 0.9240 - val_loss: 0.3261 - val_accuracy: 0.8879\n",
      "Epoch 124/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2013 - accuracy: 0.9236 - val_loss: 0.3259 - val_accuracy: 0.8873\n",
      "Epoch 125/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1985 - accuracy: 0.9232 - val_loss: 0.3259 - val_accuracy: 0.8873\n",
      "Epoch 126/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2000 - accuracy: 0.9248 - val_loss: 0.3262 - val_accuracy: 0.8873\n",
      "Epoch 127/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1994 - accuracy: 0.9240 - val_loss: 0.3261 - val_accuracy: 0.8873\n",
      "Epoch 128/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2016 - accuracy: 0.9236 - val_loss: 0.3262 - val_accuracy: 0.8873\n",
      "Epoch 129/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1991 - accuracy: 0.9242 - val_loss: 0.3263 - val_accuracy: 0.8873\n",
      "Epoch 130/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1984 - accuracy: 0.9240 - val_loss: 0.3264 - val_accuracy: 0.8873\n",
      "Epoch 131/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1998 - accuracy: 0.9246 - val_loss: 0.3263 - val_accuracy: 0.8873\n",
      "Epoch 132/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2041 - accuracy: 0.9222 - val_loss: 0.3263 - val_accuracy: 0.8873\n",
      "Epoch 133/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1958 - accuracy: 0.9246 - val_loss: 0.3263 - val_accuracy: 0.8873\n",
      "Epoch 134/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1958 - accuracy: 0.9252 - val_loss: 0.3267 - val_accuracy: 0.8873\n",
      "Epoch 135/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2008 - accuracy: 0.9238 - val_loss: 0.3267 - val_accuracy: 0.8873\n",
      "Epoch 136/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1984 - accuracy: 0.9240 - val_loss: 0.3265 - val_accuracy: 0.8873\n",
      "Epoch 137/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2046 - accuracy: 0.9244 - val_loss: 0.3266 - val_accuracy: 0.8873\n",
      "Epoch 138/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2025 - accuracy: 0.9242 - val_loss: 0.3265 - val_accuracy: 0.8873\n",
      "Epoch 139/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2007 - accuracy: 0.9238 - val_loss: 0.3270 - val_accuracy: 0.8873\n",
      "Epoch 140/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1996 - accuracy: 0.9248 - val_loss: 0.3271 - val_accuracy: 0.8873\n",
      "Epoch 141/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1981 - accuracy: 0.9252 - val_loss: 0.3272 - val_accuracy: 0.8879\n",
      "Epoch 142/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2002 - accuracy: 0.9254 - val_loss: 0.3272 - val_accuracy: 0.8873\n",
      "Epoch 143/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2029 - accuracy: 0.9238 - val_loss: 0.3269 - val_accuracy: 0.8873\n",
      "Epoch 144/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1983 - accuracy: 0.9248 - val_loss: 0.3269 - val_accuracy: 0.8873\n",
      "Epoch 145/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1967 - accuracy: 0.9254 - val_loss: 0.3271 - val_accuracy: 0.8873\n",
      "Epoch 146/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1962 - accuracy: 0.9248 - val_loss: 0.3273 - val_accuracy: 0.8879\n",
      "Epoch 147/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1997 - accuracy: 0.9248 - val_loss: 0.3276 - val_accuracy: 0.8879\n",
      "Epoch 148/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1977 - accuracy: 0.9246 - val_loss: 0.3276 - val_accuracy: 0.8879\n",
      "Epoch 149/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1988 - accuracy: 0.9250 - val_loss: 0.3277 - val_accuracy: 0.8879\n",
      "Epoch 150/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2000 - accuracy: 0.9242 - val_loss: 0.3277 - val_accuracy: 0.8879\n",
      "Epoch 151/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1981 - accuracy: 0.9252 - val_loss: 0.3274 - val_accuracy: 0.8873\n",
      "Epoch 152/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2002 - accuracy: 0.9236 - val_loss: 0.3274 - val_accuracy: 0.8873\n",
      "Epoch 153/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1971 - accuracy: 0.9248 - val_loss: 0.3277 - val_accuracy: 0.8873\n",
      "Epoch 154/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2009 - accuracy: 0.9240 - val_loss: 0.3278 - val_accuracy: 0.8873\n",
      "Epoch 155/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2008 - accuracy: 0.9242 - val_loss: 0.3277 - val_accuracy: 0.8873\n",
      "Epoch 156/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1984 - accuracy: 0.9248 - val_loss: 0.3278 - val_accuracy: 0.8873\n",
      "Epoch 157/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1963 - accuracy: 0.9248 - val_loss: 0.3280 - val_accuracy: 0.8873\n",
      "Epoch 158/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1992 - accuracy: 0.9254 - val_loss: 0.3281 - val_accuracy: 0.8873\n",
      "Epoch 159/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2013 - accuracy: 0.9240 - val_loss: 0.3282 - val_accuracy: 0.8873\n",
      "Epoch 160/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1967 - accuracy: 0.9254 - val_loss: 0.3281 - val_accuracy: 0.8873\n",
      "Epoch 161/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2010 - accuracy: 0.9248 - val_loss: 0.3280 - val_accuracy: 0.8866\n",
      "Epoch 162/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1975 - accuracy: 0.9240 - val_loss: 0.3281 - val_accuracy: 0.8873\n",
      "Epoch 163/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1984 - accuracy: 0.9240 - val_loss: 0.3281 - val_accuracy: 0.8866\n",
      "Epoch 164/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1997 - accuracy: 0.9252 - val_loss: 0.3282 - val_accuracy: 0.8873\n",
      "Epoch 165/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1978 - accuracy: 0.9250 - val_loss: 0.3284 - val_accuracy: 0.8873\n",
      "Epoch 166/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1990 - accuracy: 0.9248 - val_loss: 0.3286 - val_accuracy: 0.8873\n",
      "Epoch 167/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1974 - accuracy: 0.9240 - val_loss: 0.3286 - val_accuracy: 0.8866\n",
      "Epoch 168/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1959 - accuracy: 0.9242 - val_loss: 0.3289 - val_accuracy: 0.8873\n",
      "Epoch 169/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1999 - accuracy: 0.9232 - val_loss: 0.3288 - val_accuracy: 0.8873\n",
      "Epoch 170/200\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.1919 - accuracy: 0.92 - 0s 1ms/step - loss: 0.1948 - accuracy: 0.9246 - val_loss: 0.3288 - val_accuracy: 0.8873\n",
      "Epoch 171/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1978 - accuracy: 0.9250 - val_loss: 0.3291 - val_accuracy: 0.8873\n",
      "Epoch 172/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2001 - accuracy: 0.9238 - val_loss: 0.3290 - val_accuracy: 0.8873\n",
      "Epoch 173/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1983 - accuracy: 0.9236 - val_loss: 0.3293 - val_accuracy: 0.8873\n",
      "Epoch 174/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1994 - accuracy: 0.9244 - val_loss: 0.3291 - val_accuracy: 0.8873\n",
      "Epoch 175/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1980 - accuracy: 0.9246 - val_loss: 0.3292 - val_accuracy: 0.8873\n",
      "Epoch 176/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2040 - accuracy: 0.9244 - val_loss: 0.3292 - val_accuracy: 0.8873\n",
      "Epoch 177/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1993 - accuracy: 0.9250 - val_loss: 0.3293 - val_accuracy: 0.8873\n",
      "Epoch 178/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1991 - accuracy: 0.9248 - val_loss: 0.3293 - val_accuracy: 0.8873\n",
      "Epoch 179/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1977 - accuracy: 0.9240 - val_loss: 0.3294 - val_accuracy: 0.8873\n",
      "Epoch 180/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1966 - accuracy: 0.9234 - val_loss: 0.3293 - val_accuracy: 0.8873\n",
      "Epoch 181/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1999 - accuracy: 0.9252 - val_loss: 0.3294 - val_accuracy: 0.8873\n",
      "Epoch 182/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2022 - accuracy: 0.9242 - val_loss: 0.3295 - val_accuracy: 0.8873\n",
      "Epoch 183/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2021 - accuracy: 0.9252 - val_loss: 0.3295 - val_accuracy: 0.8866\n",
      "Epoch 184/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1980 - accuracy: 0.9252 - val_loss: 0.3298 - val_accuracy: 0.8866\n",
      "Epoch 185/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1983 - accuracy: 0.9248 - val_loss: 0.3300 - val_accuracy: 0.8873\n",
      "Epoch 186/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1986 - accuracy: 0.9244 - val_loss: 0.3300 - val_accuracy: 0.8873\n",
      "Epoch 187/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1977 - accuracy: 0.9250 - val_loss: 0.3301 - val_accuracy: 0.8873\n",
      "Epoch 188/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2014 - accuracy: 0.9240 - val_loss: 0.3301 - val_accuracy: 0.8873\n",
      "Epoch 189/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1977 - accuracy: 0.9246 - val_loss: 0.3303 - val_accuracy: 0.8873\n",
      "Epoch 190/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1997 - accuracy: 0.9242 - val_loss: 0.3304 - val_accuracy: 0.8873\n",
      "Epoch 191/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2010 - accuracy: 0.9246 - val_loss: 0.3303 - val_accuracy: 0.8873\n",
      "Epoch 192/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1968 - accuracy: 0.9244 - val_loss: 0.3304 - val_accuracy: 0.8873\n",
      "Epoch 193/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1977 - accuracy: 0.9261 - val_loss: 0.3306 - val_accuracy: 0.8873\n",
      "Epoch 194/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1972 - accuracy: 0.9248 - val_loss: 0.3306 - val_accuracy: 0.8873\n",
      "Epoch 195/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.2024 - accuracy: 0.9257 - val_loss: 0.3303 - val_accuracy: 0.8866\n",
      "Epoch 196/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1989 - accuracy: 0.9238 - val_loss: 0.3305 - val_accuracy: 0.8866\n",
      "Epoch 197/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1958 - accuracy: 0.9257 - val_loss: 0.3305 - val_accuracy: 0.8866\n",
      "Epoch 198/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1966 - accuracy: 0.9254 - val_loss: 0.3306 - val_accuracy: 0.8866\n",
      "Epoch 199/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1961 - accuracy: 0.9252 - val_loss: 0.3308 - val_accuracy: 0.8866\n",
      "Epoch 200/200\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1982 - accuracy: 0.9248 - val_loss: 0.3309 - val_accuracy: 0.8866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cf81b65700>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=200, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:38:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
       "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating ML classifiers and fitting them\n",
    "\n",
    "lr = LogisticRegression()\n",
    "svm = SVC()\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "lr.fit(x_train,y_train)\n",
    "svm.fit(x_train,y_train)\n",
    "xgb.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_test_pred = lr.predict(x_test)\n",
    "svm_test_pred = svm.predict(x_test)\n",
    "xgb_test_pred = xgb.predict(x_test)\n",
    "\n",
    "lr_acc = accuracy_score(y_test,lr_test_pred)\n",
    "svm_acc = accuracy_score(y_test,svm_test_pred)\n",
    "xgb_acc = accuracy_score(y_test,xgb_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR accuracy:  0.8952205882352942\n",
      "SVM accuracy:  0.9656862745098039\n",
      "XGB accuracy:  0.9675245098039216\n"
     ]
    }
   ],
   "source": [
    "#Accuracy on test data for ML classifiers\n",
    "\n",
    "print(\"LR accuracy: \",lr_acc)\n",
    "print(\"SVM accuracy: \",svm_acc)\n",
    "print(\"XGB accuracy: \",xgb_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating data for testing from completely unkown audio sources\n",
    "\n",
    "paths_test = ['C:/Users/sdiva/Documents/Python_scripts/RI/audio/test/yaman_test/',\n",
    "         'C:/Users/sdiva/Documents/Python_scripts/RI/audio/test/bhairavi_test/']\n",
    "\n",
    "x_data_test = []\n",
    "len_perclass_test = []\n",
    "for path in paths_test:\n",
    "    g = []\n",
    "    for (dirpath, dirnames, filenames) in walk(path):\n",
    "        g.extend(filenames)\n",
    "        break\n",
    "    for file in g:\n",
    "        s, _ = librosa.load(path+file,sr = SAMPLING_RATE)\n",
    "        row = create_x_row(s,SAMPLE_ROW_LEN,SAMPLING_RATE,JUMP,BINS)\n",
    "        x_data_test.extend(row)\n",
    "    len_perclass_test.append(len(x_data_test))\n",
    "\n",
    "x_data_test = np.array(x_data_test)\n",
    "y0_t = np.zeros(len_perclass_test[0]).reshape(-1,1)\n",
    "y1_T = np.ones(len_perclass_test[1]-len_perclass_test[0]).reshape(-1,1)\n",
    "y_data_test = np.vstack((y0_t,y1_T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[152, 300]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking calss distribution\n",
    "\n",
    "len_perclass_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 1)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting based on DNN\n",
    "\n",
    "pred_data_test = best_model.predict(x_data_test)\n",
    "pred_data_test = [0 if x < 0.5 else 1 for x in pred_data_test]\n",
    "y_data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6066666666666667"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_data_test,pred_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicitng on unkown test data with ML classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_data_test,lr.predict(x_data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6466666666666666"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_data_test,svm.predict(x_data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5966666666666667"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_data_test,xgb.predict(x_data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(signal,sample_row_len,rate,model,jump,bins):\n",
    "    \"\"\"\n",
    "    Given an audio file, function predicts class for the entire file based on the median result of its samples\n",
    "    i.e, if audio file is of 60 sec, with row-size 10 sec and jump 5 sec, there will be 11 samples of 10 sec each,\n",
    "    this function will predict class based on the median result of all 11 samples\n",
    "    \"\"\"\n",
    "    num = len(signal)//sample_row_len\n",
    "    row = []\n",
    "    for i in range(0,len(signal)-sample_row_len,jump):\n",
    "        samples = np.array(signal[i:i+sample_row_len])\n",
    "        r_spec = convert_to_spec(samples,FRAME_SIZE,HOP)\n",
    "        max_f = r_spec.argmax(axis=0)\n",
    "        count_r = np.zeros(len(bins))\n",
    "        for j in max_f:\n",
    "            count_r[j]+=1\n",
    "        pred = best_model.predict(count_r.reshape(1,-1))\n",
    "        row.append(pred)\n",
    "    med = np.median(row)\n",
    "    pred_class = 0 if med < 0.5 else 1\n",
    "    return pred_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data for predictions\n",
    "\n",
    "paths_test = ['C:/Users/sdiva/Documents/Python_scripts/RI/audio/test/yaman_test/',\n",
    "         'C:/Users/sdiva/Documents/Python_scripts/RI/audio/test/bhairavi_test/']\n",
    "predictions = []\n",
    "for path in paths_test:\n",
    "    g = []\n",
    "    for (dirpath, dirnames, filenames) in walk(path):\n",
    "        g.extend(filenames)\n",
    "        break\n",
    "    for file in g:\n",
    "        s, _ = librosa.load(path+file,sr = SAMPLING_RATE)\n",
    "        pred = predict_class(s,SAMPLE_ROW_LEN,SAMPLING_RATE,model,JUMP,BINS)\n",
    "        predictions.append(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# paths_test = ['C:/Users/sdiva/Documents/Python_scripts/RI/audio/test/yaman_test/',\n",
    "#          'C:/Users/sdiva/Documents/Python_scripts/RI/audio/test/bhairavi_test/']\n",
    "\n",
    "# Total no. audio files in the two test paths is 16, so 16 predictions below. \n",
    "# 1-8 are yaman, 9-16 are bhairavi files. \n",
    "# 0 -> predicted as yaman, 1 -> predicted as bhairavi\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = load_model(\"C:/Users/sdiva/Documents/Python_scripts/RI/models/richest_data1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saved the model which get the best predictions for unkown audio files\n",
    "\n",
    "model.save(\"C:/Users/sdiva/Documents/Python_scripts/RI/models/richest_data1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources - \n",
    "\n",
    "https://www.linkedin.com/pulse/identifying-ragas-carnatic-music-machine-learning-sridhar-ravikoti/\n",
    "\n",
    "https://www.youtube.com/watch?v=iCwMQJnKk2c&list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
